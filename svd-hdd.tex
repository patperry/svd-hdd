\documentclass{eoeauth}

\DeclareGraphicsExtensions{.png,.pdf}
\graphicspath{{plots/}}


\newcommand{\Normal}{\mathrm{Normal }}
\newcommand{\reals}{\mathbb{R}}
\newtheorem{corol}{Corollary}

\begin{document}

\title{The Singular Value Decomposition and High-Dimensional Data}{SVD and
High-Dimensional Data}

\author{G.\ Allen and P.\ O.\ Perry}

\affiliation{MRW Production Department, John Wiley \& Sons, Ltd.,
The Atrium, Southern Gate, Chichester,\\
West Sussex, PO19~1UD, U.K.}

\begin{abstract}
Blah blah blas\ldots
\end{abstract}

\begin{keywords}
keyword1, keyword2, keyword2
\end{keywords}

\section{Introduction}

A high-dimensional dataset with $n$ measurements on $p$ variables can
be represented by an $n \times p$ data matrix $X$.  This matrix gets
used, e.g. as a design matrix, modeling an external response $y$ as $y
= X \beta + \varepsilon$ for some unknown coefficient and noise vectors
$\beta$ and $\varepsilon$.  Alternatively, we may be interested in
descriptive statistics of the dataset, which can be computed in terms
of operations on the matrix $X$.  It is often desirable to work with
$\hat X$, a low-rank approximation of the data matrix instead
of working with $X$ directly.  We may either consider $\hat X$ to be a
descriptive statistic or $X$, or we may consider $\hat X$ to be a
filtered or noise-reduced version of $X$.

The most prevalent low-rank matrix approximations are based on the
singular value decomposition (SVD).  Given $X$, an $n \times p$ data
matrix, the SVD factorizes $X$ as $X = U D V'$, where
$U \in \reals^{n \times n}$ and $V \in \reals^{p \times p}$ are
orthogonal matrices and $D \in \reals^{n \times p}$ is zero except on
its diagonal.  The diagonal entries of $D$ are in decreasing order.
For $1 \leq k \leq \min(n,p)$ define $d_k = D_{kk}$ to be the $k$th
singular value of~$X$.  Similarly, for define $u_k$ to be the $k$th
column of $U$ and define $v_k$ to be the $k$th column of $V$; these
are called the $k$th left and right singular vectors of $X$.  For
$0 \leq K \leq \min(n,p)$, define the rank-$K$ matrix
\[
  \hat X_K = \sum_{k=1}^{K} d_k u_k v_k',
\]
noting that $\hat X_{\min(n,p)} = X$.  It is well known that $\hat
X_K$ is the best rank-$K$ approximation of $X$, in the sense that it
minimizes the norm $\| X - \hat X \|$ for $\hat X$ any rank-$K$ matrix
and $\| \cdot \|$ denoting either spectral or Frobenius norm.

[GENEVERA: ADD PARAGRAPH ABOUT PENALIZED DECOMPOSITIONS]

To fully understand the implications of using the singular value
decomposition in a data-processing application, we need to understand
how the SVD of $X$ behaves when the elements of $X$ are random.


\section{Random Matrix Theory for the SVD}

There are two regimes of interest for random data matrices.  In the
first regime, the number of samples, $n$, is large relative to the number of
variables, $p$, and in the second regime the two numbers are comparable.
We call the first regime the ``classical'' regime and the second
regime the ``modern'' regime.  The classical regime is caracterized by
$n \to \infty$ and $p$ fixed; the modern regime is characterized by $n
\to \infty$, $p \to \infty$, and $n/p \to \gamma$, where $\gamma$ is a
fixed scalar in $(0,\infty)$.  The results we summarize consider the
columns of $X'$, denoted $x_1, \dotsc, x_n$, to be independent and
identically distributed (IID) observations.  We state the results for
Gaussian random variables, but many of the results hold for
arbitrary distributions with finite fourth moments.  Throughout, we
assume that $x_1, x_2, \dotsc, x_n$ are independent $\Normal(0, \Sigma)$ random
variables for some $p \times p$ covariance matrix~$\Sigma$, and that
\(
  X' =
  \begin{bmatrix}
    x_1 & x_2 & \dots & x_n
  \end{bmatrix}.
\)

The singular value decomposition of $X$ is closely related to the
eigendecomposition of $X' \, X$.  Specifically, if $U D V'$ is an SVD
of $X$, then $V \, (D' D) \, V'$ is an eigendecomposition of $X' X$.
Thus, the eigenvalues of $X' X$ are the squares of the singular values
of $X$, and the eigenvectors of $X' X$ are the right singular vectors
of $X$.  Conversely, if $X' X = V \Lambda V'$ is an eigendecomposition of
$X' X$, then the first $\min(n,p)$ left singular vectors of $X$ are the
first $\min(n,p)$ columns of $X V \Lambda^\dagger$, where
$\Lambda^\dagger$ denotes the Moore-Penrose pseudoinverse of $\Lambda$.
With these identities, the SVD of $X$ can be analyzed in terms of the
eigendecomposition of $X' X$.


\subsection{Classical results}
[PATRICK]

\subsection{Bulk of the spectrum}
[PATRICK]

\subsection{Edge of the spectrum}
[PATRICK]

\subsection{Eigenvectors}
[PATRICK]

\subsection{Results for spike eigenvectors}
[PATRICK]

\section{Penalized SVDs (?)}
[GENEVERA]

\section{Application/Example (?)}

\section{Conclusion}



\end{document}