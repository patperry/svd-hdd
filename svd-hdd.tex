\documentclass{eoeauth}

\DeclareGraphicsExtensions{.png,.pdf}
\graphicspath{{plots/}}


\newcommand{\Normal}{\mathrm{Normal }}
\newcommand{\reals}{\mathbb{R}}
\newtheorem{corol}{Corollary}

\begin{document}

\title{The Singular Value Decomposition and High-Dimensional Data}{SVD and
High-Dimensional Data}

\author{G.\ Allen and P.\ O.\ Perry}

\affiliation{MRW Production Department, John Wiley \& Sons, Ltd.,
The Atrium, Southern Gate, Chichester,\\
West Sussex, PO19~1UD, U.K.}

\begin{abstract}
Blah blah blas\ldots
\end{abstract}

\begin{keywords}
keyword1, keyword2, keyword2
\end{keywords}

\section{Introduction}

A high-dimensional dataset with $n$ measurements on $p$ variables can
be represented by an $n \times p$ data matrix $X$.  This matrix gets
used, e.g. as a design matrix, modeling an external response $y$ as $y
= X \beta + \varepsilon$ for some unknown coefficient and noise vectors
$\beta$ and $\varepsilon$.  Alternatively, we may be interested in
descriptive statistics of the dataset, which can be computed in terms
of operations on the matrix $X$.  It is often desirable to work with
$\hat X$, a low-rank approximation of the data matrix instead
of working with $X$ directly.  We may either consider $\hat X$ to be a
descriptive statistic or $X$, or we may consider $\hat X$ to be a
filtered or noise-reduced version of $X$.

The most prevalent low-rank matrix approximations are based on the
singular value decomposition (SVD).  Given $X$, an $n \times p$ data
matrix, the SVD factorizes $X$ as $X = U D V'$, where
$U \in \reals^{n \times n}$ and $V \in \reals^{p \times p}$ are
orthogonal matrices and $D \in \reals^{n \times p}$ is zero except on
its diagonal.  The diagonal entries of $D$ are in decreasing order.
For $1 \leq k \leq \min(n,p)$ define $d_k = D_{kk}$ to be the $k$th
singular value of~$X$.  Similarly, for define $u_k$ to be the $k$th
column of $U$ and define $v_k$ to be the $k$th column of $V$; these
are called the $k$th left and right singular vectors of $X$.  For
$0 \leq K \leq \min(n,p)$, define the rank-$K$ matrix
\[
  \hat X_K = \sum_{k=1}^{K} d_k u_k v_k',
\]
noting that $\hat X_{\min(n,p)} = X$.  It is well known that $\hat
X_K$ is the best rank-$K$ approximation of $X$, in the sense that it
minimizes the norm $\| X - \hat X \|$ for $\hat X$ any rank-$K$ matrix
and $\| \cdot \|$ denoting either spectral or Frobenius norm.

[GENEVERA: ADD PARAGRAPH ABOUT PENALIZED DECOMPOSITIONS]

To fully understand the implications of using the singular value
decomposition in a data-processing application, we need to understand
how the SVD of $X$ behaves when the elements of $X$ are random.


\section{Random Matrix Theory for the SVD}

There are two regimes of interest for random data matrices.  In the
first regime, the number of samples, $n$, is large relative to the number of
variables, $p$, and in the second regime the two numbers are comparable.
We call the first regime the ``classical'' regime and the second
regime the ``modern'' regime.  The classical regime is caracterized by
$n \to \infty$ and $p$ fixed; the modern regime is characterized by $n
\to \infty$, $p \to \infty$, and $n/p \to \gamma$, where $\gamma$ is a
fixed scalar in $(0,\infty)$.  The results we summarize consider the
columns of $X'$, denoted $x_1, \dotsc, x_n$, to be independent and
identically distributed (IID) observations.  We state the results for
Gaussian random variables, but many of the results hold for
arbitrary distributions with finite fourth moments.  Throughout, we
assume that $x_1, x_2, \dotsc, x_n$ are independent $\Normal(0, \Sigma)$ random
variables for some $p \times p$ covariance matrix~$\Sigma$, and that
\(
  X' =
  \begin{bmatrix}
    x_1 & x_2 & \dots & x_n
  \end{bmatrix}.
\)

The singular value decomposition of $X$ is closely related to the
eigendecomposition of $X' \, X$.  Specifically, if $U D V'$ is an SVD
of $X$, then $V \, (D' D) \, V'$ is an eigendecomposition of $X' X$.
Thus, the eigenvalues of $X' X$ are the squares of the singular values
of $X$, and the eigenvectors of $X' X$ are the right singular vectors
of $X$.  Conversely, if $X' X = V \Lambda V'$ is an eigendecomposition of
$X' X$, then the first $\min(n,p)$ left singular vectors of $X$ are the
first $\min(n,p)$ columns of $X V \Lambda^\dagger$, where
$\Lambda^\dagger$ denotes the Moore-Penrose pseudoinverse of $\Lambda$.
With these identities, the SVD of $X$ can be analyzed in terms of the
eigendecomposition of $X' X$.

Using properties of the Gaussian distribution, one
can show that for any fixed $n\times n$ orthogonal matrix $O$, the
product $O X$ has the same distribution as $X$.  Thus, the
distribution of the left singular vectors of $X$ is invariant under
multiplication on the left by an orthogonal matrix, i.e. $O U
\overset{d}{=} U$ for any fixed orthgonal $O$.  Thus, the
first $\min(n,p)$ columns of $U$ must have the same distribution as
the first $\min(n,p)$ columns of a uniformly-distributed (Haar)
orthogonal matrix.  Now, $X' X$ is a complete sufficient statistic
for $\Sigma$ and $U$ is ancilarly, so by Basu's theorem, $U$ is
independent of $D$ and $V$.  Since we have just derived the
distribution of $U$, the distribution of $X' X$ completely
characterizes the behavior of the SVD of $X$.

In the sequel, we let $\Sigma = \Phi \Lambda \Phi'$ be an
eigendecomposition of $\Sigma$.  We assume that the eigenvalues of
$\Sigma$ are ordered as $\lambda_1 \geq \lambda_2 \geq \dotsc \geq \lambda_p \geq 0$.
The eigendecomposition of $\Sigma$ is not unique since we can multiply
a column of $\Phi$ by $-1$ and still get the same product $\Phi
\Lambda \Phi'$; more generally, we can arbitrarily rotate
eigenspaces.  The same indeterminacy problems exist for $V$.  To deal
with these indeterminancies, when we state results for the
eigendecomposition of $X' X$ we assume that the signs of the columns
are chosen such that $\varphi_j ' v_j \, > 0$.


\subsection{Classical results}

In the classical regime, $p$ is fixed and $n$ increases
asymptotically.  By the strong law of large numbers, $\tfrac{1}{n} X'
X \overset{\text{a.s.}}{\to} \Sigma$, so the eigendecomposition of
$\tfrac{1}{n} X' X$ converges to that of $\Sigma$ (up to
identifiability constraints).
Anderson derived the distribution of $X' X$ when $p$ is fixed and $n$
increases asymptotically \cite{anderson63}.  When the eigenvalues of $\Sigma$ are
distinct, his result implies the following: after appropriate cenering
and scaling, $D^2$ and $V$ converge jointly in distribution and their limits are independent;
tor all $1 \leq j \leq p$
\[
  \frac{d^2_{j} - n \lambda_j}{\sqrt{n}}
    \overset{d}{\to}
      \mathrm{Normal}\left( 0, \, 2 \lambda_j^2 \right),
\qquad\text{and}\qquad
   \sqrt{n} \left( \Phi' V - I \right) \overset{d}{\to} F,
\]
where $F$ is a skew-symmetric matrix with elements above the 
diagonal independent of each other and distributed as 
\(
  F_{jk}
    \sim
      \mathrm{Normal} \left(
        0, \,
        \frac{\lambda_j \lambda_k}
        {\left( \lambda_j - \lambda_k \right)^2}
      \right),
      \quad
      \text{for all $1 \leq j < k \leq p$.}
\)
Anderson's result is more general, and handles the case when the
$\lambda_j$ are not all unique.


\subsection{Modern results}

In the modern asymptotic regime, the dimension increases with the
sample size $n$, so that the sequence of column dimensions $p_n$
replaces the fixed column dimension $p$, and the sequence of
covariance matrices $\Sigma_n$ replaces the fixed covariance matrix $\Sigma$.
Even though  $\tfrac{1}{n} X' X$ converges elementwise to $\Sigma_n$,
the eigendecomposition of the former does not converge to the
eigendecomposition of the latter.  Even restricting attention to the
top $K$ eigenvalue-eigenvector pairs, the sample quantities are inconsistent.


In the ``null'' case, when $\Sigma_n = I$ and $\tfrac{n}{p} \to \gamma$,
the scaled eigenvalue $\frac{d_1^2}{n}$ converges almost surely to
$(1 + \gamma^{-1/2})^2$ \cite{geman1980ltn,jonsson1983ole}.  Similar
behavior (asymptotic upward bias) holds with a different limit for
non-identity $\Sigma_n$
\cite{silverstein1984ole,yin1988lle,bai1988nle}. 

\subsubsection{The Null Case}

Johnstone \cite{johnstone2001dle} and Johansson \cite{johansson2000sfa}
and derived the limiting null distribution (after appropriate
centering and scaling) of $d_1^2$ for real and complex data when
$\Sigma_n = I$.  With center
$\mu_n = \left(\sqrt{n-\tfrac{1}{2}} + \sqrt{p_n - \tfrac{1}{2}}\right)^2$ and scale
$\sigma_n = \left(\sqrt{n-\tfrac{1}{2}} + \sqrt{p_n -
    \tfrac{1}{2}}\right) \left(1/\sqrt{n-\tfrac{1}{2}} + 1/\sqrt{p_n -
    \tfrac{1}{2}}\right)^{1/3}$, 
the quantity
\(
\frac{d^2_1 - \mu_n}{\sigma_n}
\)
converges in distribution to a random variable with the Tracy-Widom
law of order $1$.  This distribution first appeared as the limit
(after appropriate centering and scaling) of the maximum eigenvalue
from an $n \times n$ symmetric matrix with idepending Gaussian entries
having variance $2$ on the diagonal and variance $1$ otherwise
\cite{tracy1994lsd, tracy1996oas}.

\subsubsection{The Alternative Case}
 The popular ``alternative'' case is when the eigenvalues of $\Sigma_n$ are
``spiked,'' so that the top few eigenvalues are larger than the rest.



\section{Penalized SVDs (?)}
[GENEVERA]

\section{Application/Example (?)}

\section{Conclusion}

\bibliographystyle{plain}
\bibliography{sources,perry-thesis-refs}

\end{document}